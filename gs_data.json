{"container_type": "Author", "filled": ["basics", "publications", "indices", "counts"], "scholar_id": "4nAZQDwAAAAJ", "source": "AUTHOR_PROFILE_PAGE", "name": "Xiaoyu Gong", "url_picture": "https://scholar.googleusercontent.com/citations?view_op=view_photo&user=4nAZQDwAAAAJ&citpid=1", "affiliation": "Jilin University", "organization": 16073379652311030762, "interests": ["Reinforcement Learning、Deep Learning"], "email_domain": "@mails.jlu.edu.cn", "citedby": 51, "publications": {"4nAZQDwAAAAJ:u-x6o8ySG0sC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Actor-critic with familiarity-based trajectory experience replay", "pub_year": "2022"}, "filled": false, "author_pub_id": "4nAZQDwAAAAJ:u-x6o8ySG0sC", "num_citations": 21, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=18441076035699623023", "cites_id": ["18441076035699623023"]}, "4nAZQDwAAAAJ:u5HHmVD_uO8C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Evolutionary generative adversarial networks with crossover based knowledge distillation", "pub_year": "2021"}, "filled": false, "author_pub_id": "4nAZQDwAAAAJ:u5HHmVD_uO8C", "num_citations": 14, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=3260335885366140160", "cites_id": ["3260335885366140160"]}, "4nAZQDwAAAAJ:d1gkVwhDpl0C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Entropy regularization methods for parameter space exploration", "pub_year": "2023"}, "filled": false, "author_pub_id": "4nAZQDwAAAAJ:d1gkVwhDpl0C", "num_citations": 5, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=17850454626182984511", "cites_id": ["17850454626182984511"]}, "4nAZQDwAAAAJ:UeHWp8X0CEIC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Adaptive estimation Q-learning with uncertainty and familiarity", "pub_year": "2023"}, "filled": false, "author_pub_id": "4nAZQDwAAAAJ:UeHWp8X0CEIC", "num_citations": 4, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=16213553123747907166", "cites_id": ["16213553123747907166"]}, "4nAZQDwAAAAJ:qjMakFHDy7sC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Guided deterministic policy optimization with gradient-free policy parameters information", "pub_year": "2023"}, "filled": false, "author_pub_id": "4nAZQDwAAAAJ:qjMakFHDy7sC", "num_citations": 3, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=3459574565753566794", "cites_id": ["3459574565753566794"]}, "4nAZQDwAAAAJ:2osOgNQ5qMEC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Survey on Deep Reinforcement Learning Methods Based on Sample Efficiency Optimization", "pub_year": "2021"}, "filled": false, "author_pub_id": "4nAZQDwAAAAJ:2osOgNQ5qMEC", "num_citations": 2, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=2350768700447646662", "cites_id": ["2350768700447646662"]}, "4nAZQDwAAAAJ:9yKSN-GCB0IC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "基于样本效率优化的深度强化学习方法综述", "pub_year": "2021"}, "filled": false, "author_pub_id": "4nAZQDwAAAAJ:9yKSN-GCB0IC", "num_citations": 2, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=2636592987358242298", "cites_id": ["2636592987358242298"]}}, "citedby5y": 51, "hindex": 4, "hindex5y": 4, "i10index": 2, "i10index5y": 2, "cites_per_year": {"2021": 2, "2022": 8, "2023": 14, "2024": 20, "2025": 7}, "updated": "2025-02-27 08:25:24.468738"}